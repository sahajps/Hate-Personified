{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "633456f2-7524-4870-95c0-a5406aa3cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da5b037a-2646-47e0-9bd0-b6cf2091a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_fn(row):\n",
    "    row = str(row)\n",
    "    if(re.search('yes', row) and re.search('no', row)):\n",
    "        return None\n",
    "    elif(re.search('yes', row)):\n",
    "        return 0\n",
    "    elif(re.search('non-hateful', row)):\n",
    "        return 0\n",
    "    elif(re.search('no', row)):\n",
    "        return 1\n",
    "    elif(re.search('hateful', row)):\n",
    "        return 0\n",
    "    elif(re.search('hate.', row)):\n",
    "        return 0\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20022a3a-6c64-4838-9a99-6c802c1e14a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:16<00:00,  2.79s/it]\n",
      "100%|██████████| 6/6 [00:00<00:00, 14.19it/s]\n"
     ]
    }
   ],
   "source": [
    "files = ['P_base'] + ['P1_'+str(i) for i in range(1,6)]\n",
    "\n",
    "type = ['OS_LLMs', 'ChatGPT']\n",
    "for t in type:\n",
    "    for f in tqdm(files):\n",
    "        df = pd.read_excel('../'+t+'/'+f+'.xlsx')\n",
    "\n",
    "        # For FlanT5-XXL, you can choose the below 500 samples and print at the end to check it's performance to compare with ChatGPT\n",
    "        # Also, keep `files = ['P_base']` and `type = ['OS_LLMs']`\n",
    "        #df = df.sample(n=500, random_state=42)\n",
    "        col = df.columns[2:]\n",
    "        \n",
    "        no_ans = []\n",
    "        F1 = []\n",
    "        IAA = []\n",
    "        \n",
    "        for i in col:\n",
    "            df[i] = df[i].apply(lambda x: str(x).lower())\n",
    "            \n",
    "            df[i] = df[i].apply(map_fn)\n",
    "    \n",
    "            pen = (df.shape[0]-df[i].isna().sum())/df.shape[0]\n",
    "            no_ans.append(df[i].isna().sum())\n",
    "    \n",
    "            tmp = df[['label', i]].dropna()\n",
    "    \n",
    "            F1.append(pen*f1_score(tmp['label'], tmp[i], average='weighted'))\n",
    "        \n",
    "            IAA.append(pen*cohen_kappa_score(tmp['label'], tmp[i]))\n",
    "    \n",
    "        \n",
    "        res = pd.read_csv('Performance'+t+'_H.csv')\n",
    "        try:\n",
    "            res[f+' Hal'] = no_ans\n",
    "            res[f+' F1'] = F1\n",
    "            res[f+' IAA'] = IAA\n",
    "        except:\n",
    "            # if there is no output for 8th model (Llama3) then it should append `None`\n",
    "            no_ans.append(None)\n",
    "            F1.append(None)\n",
    "            IAA.append(None)\n",
    "            res[f+' Hal'] = no_ans\n",
    "            res[f+' F1'] = F1\n",
    "            res[f+' IAA'] = IAA\n",
    "        res.to_csv('Performance'+t+'_H.csv', index=False)\n",
    "\n",
    "        # To print for a single compair (FlanT5-XXL -- in case)\n",
    "        # print(no_ans[-1], F1[-1], IAA[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a5afee7-058a-4022-aad8-12dde2a60a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.13it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 18.49it/s]\n"
     ]
    }
   ],
   "source": [
    "files = ['P2_'+str(i) for i in range(1,6)]\n",
    "type = ['OS_LLMs', 'ChatGPT']\n",
    "\n",
    "for t in type:\n",
    "    for f in tqdm(files):\n",
    "        df = pd.read_excel('../'+t+'/'+f+'.xlsx')\n",
    "        #df = df.sample(n=500, random_state=42)\n",
    "        col = df.columns[2:]\n",
    "        \n",
    "        no_ans = []\n",
    "        F1 = []\n",
    "        IAA = []\n",
    "        \n",
    "        for i in col:\n",
    "            df[i] = df[i].apply(lambda x: str(x).lower())\n",
    "            \n",
    "            df[i] = df[i].apply(map_fn)\n",
    "    \n",
    "            pen = (df.shape[0]-df[i].isna().sum())/df.shape[0]\n",
    "            no_ans.append(df[i].isna().sum())\n",
    "    \n",
    "            tmp = df[['label', i]].dropna()\n",
    "    \n",
    "            F1.append(pen*f1_score(tmp['label'], tmp[i], average='weighted'))\n",
    "        \n",
    "            IAA.append(pen*cohen_kappa_score(tmp['label'], tmp[i]))\n",
    "    \n",
    "        \n",
    "        res = pd.read_csv('Performance'+t+'_N.csv')\n",
    "        res[f+' Hal'] = no_ans\n",
    "        res[f+' F1'] = F1\n",
    "        res[f+' IAA'] = IAA\n",
    "        res.to_csv('Performance'+t+'_N.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
